{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dehydrating Tweet IDs\n",
    "\n",
    "In most social media projects, it is desirable to have open access to content.\n",
    "\n",
    "Twitter has rules and regulations how one can share dataset publicly. Check out section [Content redistribution](https://developer.twitter.com/en/developer-terms/agreement-and-policy) or quotes below.\n",
    "\n",
    "```\n",
    "The best place to get Twitter Content is directly from Twitter. Consequently, we restrict the redistribution of Twitter Content to third parties.  If you provide Twitter Content to third parties, including downloadable datasets or via an API, you may only distribute Tweet IDs, Direct Message IDs, and/or User IDs (except as described below). We also grant special permissions to academic researchers sharing Tweet IDs and User IDs for non-commercial research purposes.\n",
    "\n",
    "In total, you may not distribute more than 1,500,000 Tweet IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from Twitter. In addition, all developers may provide up to 50,000 public Tweets Objects and/or User Objects to each person who uses your service on a daily basis if this is done via non-automated means (e.g., download of spreadsheets or PDFs).\n",
    "\n",
    "Academic researchers are permitted to distribute an unlimited number of Tweet IDs and/or User IDs if they are doing so on behalf of an academic institution and for the sole purpose of non-commercial research. For example, you are permitted to share an unlimited number of Tweet IDs for the purpose of enabling peer review or validation of your research. If you have questions about whether your use case qualifies under this category please submit a request via the API Policy Support form.\n",
    "```\n",
    "\n",
    "It's a common practice to share files with list of TweetIDs. I'm sharing a similar file `data/tweet-id-collection.txt` for our use in this tutorial.\n",
    "\n",
    "There are couple of ways to obtains content for these Tweets. Some applications like [Hydrator](https://github.com/DocNow/hydrator) provide easy to use interface. \n",
    "\n",
    "We can also download tweets using Twitter's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T18:43:09.111165Z",
     "start_time": "2020-06-02T18:43:09.104030Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "\n",
    "import tweepy\n",
    "\n",
    "# Update the file with your Twitter Application Credentials and DO NOT SHARE with others.\n",
    "# I include this config.py file into .gitignore file, so that my changes won't get tracked by Github.\n",
    "from config import TWITTER_KEYS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:49:34.977112Z",
     "start_time": "2020-06-02T17:49:34.971912Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(TWITTER_KEYS[\"consumer_key\"], TWITTER_KEYS[\"consumer_secret\"])\n",
    "auth.set_access_token(TWITTER_KEYS[\"access_token\"], TWITTER_KEYS[\"access_token_secret\"])            \n",
    "api = tweepy.API(auth, retry_count=5, retry_delay=15, compression=True,\n",
    "                 wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "#api.verify_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Tweet objects from TweetId file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:42:28.281561Z",
     "start_time": "2020-06-02T19:42:09.565291Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280257 tweet-ids in the file\n",
      "243533 tweets already collected\n",
      "36724 tweet-ids to collect or not available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nrequested, collected = 0, 0\\nwith gzip.open('../data/tweets_dehydrated.jsons.gz', 'wb') as fl:\\n    for idx in range(0, len(tweetIds), 100):\\n        chunk = tweetIds[idx:idx+100]\\n        for tweet in api.statuses_lookup(chunk, include_entities=True):\\n            fl.write('{}\\n'.format(json.dumps(tweet._json)).encode('utf-8'))\\n            collected += 1\\n        requested += len(chunk)\\n\\n        print('{} requested, {} collected so far'.format(requested, collected))\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetIds = list()\n",
    "with open('../data/tweet-id-collection.txt', 'r') as fl:\n",
    "    for line in fl:\n",
    "        tweetIds.append(line.strip())\n",
    "print('{} tweet-ids in the file'.format(len(tweetIds)))\n",
    "\n",
    "alreadyCollected = set()\n",
    "with gzip.open('../data/tweets_dehydrated.jsons.gz', 'rb') as fl:\n",
    "    for line in fl:\n",
    "        tweet = json.loads(line)\n",
    "        alreadyCollected.add(tweet['id_str'])\n",
    "print('{} tweets already collected'.format(len(alreadyCollected)))\n",
    "\n",
    "tweetIds = list(set(tweetIds) - alreadyCollected)\n",
    "print('{} tweet-ids to collect or not available'.format(len(tweetIds)))\n",
    "\n",
    "'''\n",
    "## UNCOMMENT IF YOU WANT TO CONTINUE DATA COLLECTION\n",
    "requested, collected = 0, 0\n",
    "with gzip.open('../data/tweets_dehydrated.jsons.gz', 'wb') as fl:\n",
    "    for idx in range(0, len(tweetIds), 100):\n",
    "        chunk = tweetIds[idx:idx+100]\n",
    "        for tweet in api.statuses_lookup(chunk, include_entities=True):\n",
    "            fl.write('{}\\n'.format(json.dumps(tweet._json)).encode('utf-8'))\n",
    "            collected += 1\n",
    "        requested += len(chunk)\n",
    "\n",
    "        print('{} requested, {} collected so far'.format(requested, collected))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Downloading data from public server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also uploaded this dataset to Zenodo.\n",
    "\n",
    "You can use [this link](http://doi.org/10.5281/zenodo.3900655) to access the file.\n",
    "\n",
    "`\n",
    "Onur Varol. (2020). SICSS - Tutorial Dataset File (Version v1.0.0) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.3900655\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
